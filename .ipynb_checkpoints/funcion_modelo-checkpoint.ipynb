{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook script_reglas.ipynb to script\n",
      "[NbConvertApp] Writing 4795 bytes to script_reglas.py\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from sklearn.externals import joblib\n",
    "import re\n",
    "import script_reglas\n",
    "import unicodedata\n",
    "from numpy import reshape, concatenate, unique, vectorize, array\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_stem_tfidf=joblib.load('./modelo/mat_tfidf.pkl') #1\n",
    "pca=joblib.load('./modelo/pca.pkl')  #2\n",
    "clasificador=joblib.load('./modelo/modelo.pkl') # 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos los pkls que se generaron en entrenamiento modelo y les aplicamos las mismas funciones al texto nuevo para que entre igual que el texto con el que se entrenó. \n",
    "\n",
    "En la función de producción de modelo construimos la tabla de features en el mismo orden en el que entró al entrenamiento: 33 factores, word count y hora. La hora del nuevo mensaje saldrá de una petición GET al api de rapidpro. Se busca el último mensaje y extra su hora (en entero del 0 al 24). \n",
    "\n",
    "Se saca la probabilidad predicha de cada clase y se suman las probabilidades que van juntas. (Se agregaron categorías para ayudar a la clasificación de mensajes. Pregunta médica y pregunta busca trabajo son ambas preguntas, pero contienen mensajes muy diferentes). \n",
    "\n",
    "#### Se agrega: \n",
    "* Pregunta:\n",
    " + Búsqueda de trabajo\n",
    " + Preguntas médicas\n",
    " + Otras Preguntas  \n",
    "* Otra:\n",
    " + Quejas\n",
    " + Otras\n",
    " \n",
    "#### Índice de concentración\n",
    "\n",
    "El modelo calcula una predicción para todos los mensajes. Sin embargo, habrá veces que dos o más clases tienen alta probabilidad. Para controlar ese caso, se calcula un índice de concentración. Entre más alto, más concentrada está la probabilidad en una sola clase. Si el índice está por debajo de 4767.621 el modelo no asignará una clase. El modelo asignará una clase a aproximadamente el 80% de los mensajes enviados. Esto resulta en una precisión fuera de muestra de 80% de los casos.\n",
    "\n",
    "#### Flag de emergencias\n",
    "\n",
    "Los mensajes que sean emergencias y el modelo no prediga correctamente, son los errores más costosos. Por ello, si la probabilidad de emergencia es mayor a 1%, se le asignará la clase cuya probabildad sea la máxima. Sin embargo, se agregará un flag que indicará que el mensaje tiene una probabilidad alta de ser emergencia. Este flag se le asigna al ~8% de los mensajes y logra captar ~60% de las emergencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map={0:'emergencia',\n",
    "           1:'informacion',\n",
    "            2:'nacimiento',\n",
    "            3:'otra',\n",
    "            4:'pregunta',\n",
    "            5:'respuesta'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def procesa_texto(texto):\n",
    "    texto=texto.lower()\n",
    "    \n",
    "    \n",
    "    part=texto.partition('http')\n",
    "    part=part[0]+part[1]+' '+ ' '.join(part[2].split(' ')[1:])\n",
    "    texto=part\n",
    "    \n",
    "    part=texto.partition('bit.ly')\n",
    "    part=part[0]+part[1]+' '+ ' '.join(part[2].split(' ')[1:])\n",
    "    texto=part\n",
    "    \n",
    "    texto=re.sub('^[ \\t]+|[ \\t]+$', '', texto) \n",
    "\n",
    "    texto=re.sub('[^\\w\\s]','', texto)\n",
    "\n",
    "    texto=re.sub('^[ \\t]+|[ \\t]+$', '', texto)\n",
    "\n",
    "    texto=script_reglas.give_emoji_free_text(texto)\n",
    "\n",
    "    texto=unicodedata.normalize('NFD', texto).encode('ascii', 'ignore').decode('utf-8')\n",
    "    wc=len(str(texto).split(\" \"))\n",
    "    texto=re.sub('ola|buena noche|buenos dias|buenos dia|buen dia|buenas noches|buenas tardes|buenas tarde|buen dia|bien dia|buena tardes|buena tarde|saludos|hola','', texto)\n",
    "    texto=re.sub('\\n',' ', texto)\n",
    "    texto=re.sub('^[ \\t]+|[ \\t]+$', '', texto)\n",
    "    return texto , wc\n",
    "\n",
    "stemmer = SnowballStemmer(\"spanish\")\n",
    "\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "\n",
    "def predice_modelo(contact_uuid, texto, token):\n",
    "    response = requests.get(\"http://rapidpro.datos.gob.mx/api/v2/runs.json\",\n",
    "                            headers={\"Authorization\": token},\n",
    "                            params={\"contact\": contact_uuid})\n",
    "    response=json.loads(response.text)\n",
    "    hora=response['results'][0]['created_on']\n",
    "    hora=int(hora[11:13])\n",
    "\n",
    "    texto, wc =procesa_texto(texto)\n",
    "    \n",
    "    texto=tokenize_and_stem(texto)\n",
    "    texto=' '.join(texto)\n",
    "    \n",
    "    wc=reshape(wc, (-1, 1))\n",
    "    hora=reshape(hora, (-1, 1))\n",
    "\n",
    "\n",
    "    tfidf=features_stem_tfidf.transform([texto])\n",
    "    features=pca.transform(tfidf)\n",
    "\n",
    "    features=concatenate((features, wc), axis=1)\n",
    "    features=concatenate((features, hora), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    proba=clasificador.predict_proba(features)\n",
    "\n",
    "    proba_orig=proba\n",
    "    \n",
    "    proba=array([proba[0][0], #'emergencia'\n",
    "    proba[0][1], #Informacion\n",
    "    proba[0][2], #Nacimiento\n",
    "    proba[0][3]+proba[0][4], #otra\n",
    "    proba[0][5]+proba[0][6]+proba[0][7], #Pregunta\n",
    "    proba[0][8]]) #Respuesta\n",
    "    \n",
    "    pred=proba.argmax()\n",
    "    \n",
    "    conc=proba*100\n",
    "    conc=conc*conc\n",
    "    conc=conc.sum()\n",
    "\n",
    "\n",
    "    pred=proba.argmax()\n",
    "    max_proba=proba.max()\n",
    "    \n",
    "\n",
    "    \n",
    "    pred=str(vectorize(label_map.get)(pred))\n",
    "    \n",
    "    \n",
    "    if conc<minimo_conc:\n",
    "        pred='No_se_puede_asignar_etiqueta'\n",
    "    if proba[0]>0.030110899:\n",
    "        pred=pred+'-FLAG'\n",
    "    \n",
    "    out={'pred':pred,\n",
    "        'probabilidad_maxima':max_proba,\n",
    "        'indice_seguridad':conc, \n",
    "        'proba':proba}\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posibles respuestas de la función y acciones esperadas: \n",
    "\n",
    "* Información -> Flujo de información\n",
    "* Nacimiento -> Flujo de nacimiento\n",
    "* Respuesta -> Flujo de respuesta\n",
    "* Otra -> Flujo de otro\n",
    "\n",
    "Si la probabilidad de emergencia es mayor a 3%:\n",
    "\n",
    "* Emergencia-FLAG -> Flujo de emergencia\n",
    "* Información-FLAG -> Mensaje precautorio -> Flujo de información\n",
    "* Nacimiento-FLAG -> Mensaje precautorio -> Flujo de nacimiento\n",
    "* Respuesta-FLAG -> Mensaje precautorio -> Flujo de respuesta\n",
    "* Otra-FLAG -> Mensaje precautorio -> Flujo de otra\n",
    "\n",
    "Si el índice de concentración está abajo de 4767.621:\n",
    "\n",
    "* No_se_puede_asignar_etiqueta -> webhook autoetiquetado\n",
    "* No_se_puede_asignar_etiqueta-FLAG -> Mensaje precautorio -> webhook autoetiquetado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##THRESHOLD DE CONCENTRACIÓN. ABAJO DE ESTO NO SE PUEDE PREDECIR\n",
    "\n",
    "minimo_conc=4767.621\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ID CONTACTO\n",
    "contact_uuid='e128652a-3a66-4fc2-98b9-36964ca1cd9b'\n",
    "#TEXTO DEL MENSAJE\n",
    "texto= \"Sólo me dijeron que fuera sí necesitaba más medicamentos o me sentía mal\"\n",
    "# TOKEN DEL API\n",
    "token=\"Token []\"\n",
    "\n",
    "predice_modelo(contact_uuid, texto, token)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook funcion_modelo.ipynb to script\n",
      "[NbConvertApp] Writing 6446 bytes to funcion_modelo.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script funcion_modelo.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from sklearn.externals import joblib\n",
    "import re\n",
    "import script_reglas\n",
    "import unicodedata\n",
    "from numpy import reshape, concatenate, unique, vectorize, array\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_stem_tfidf=joblib.load('./modelo/mat_tfidf.pkl') #1\n",
    "pca=joblib.load('./modelo/pca.pkl')  #2\n",
    "clasificador=joblib.load('./modelo/modelo.pkl') # 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def procesa_texto(texto):\n",
    "    texto=texto.lower()\n",
    "    \n",
    "    \n",
    "    part=texto.partition('http')\n",
    "    part=part[0]+part[1]+' '+ ' '.join(part[2].split(' ')[1:])\n",
    "    texto=part\n",
    "    \n",
    "    part=texto.partition('bit.ly')\n",
    "    part=part[0]+part[1]+' '+ ' '.join(part[2].split(' ')[1:])\n",
    "    texto=part\n",
    "    \n",
    "    texto=re.sub('^[ \\t]+|[ \\t]+$', '', texto) \n",
    "\n",
    "    texto=re.sub('[^\\w\\s]','', texto)\n",
    "\n",
    "    texto=re.sub('^[ \\t]+|[ \\t]+$', '', texto)\n",
    "\n",
    "    texto=script_reglas.give_emoji_free_text(texto)\n",
    "\n",
    "    texto=unicodedata.normalize('NFD', texto).encode('ascii', 'ignore').decode('utf-8')\n",
    "    wc=len(str(texto).split(\" \"))\n",
    "    texto=re.sub('ola|buena noche|buenos dias|buenos dia|buen dia|buenas noches|buenas tardes|buenas tarde|buen dia|bien dia|buena tardes|buena tarde|saludos|hola','', texto)\n",
    "    texto=re.sub('\\n',' ', texto)\n",
    "    texto=re.sub('^[ \\t]+|[ \\t]+$', '', texto)\n",
    "    return texto , wc\n",
    "\n",
    "stemmer = SnowballStemmer(\"spanish\")\n",
    "\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "\n",
    "def predice_modelo(contact_uuid, texto, token):\n",
    "    response = requests.get(\"http://rapidpro.datos.gob.mx/api/v2/runs.json\",\n",
    "                            headers={\"Authorization\": token},\n",
    "                            params={\"contact\": contact_uuid})\n",
    "    response=json.loads(response.text)\n",
    "    hora=response['results'][0]['created_on']\n",
    "    hora=int(hora[11:13])\n",
    "\n",
    "    texto, wc =procesa_texto(texto)\n",
    "    \n",
    "    texto=tokenize_and_stem(texto)\n",
    "    texto=' '.join(texto)\n",
    "    \n",
    "    wc=reshape(wc, (-1, 1))\n",
    "    hora=reshape(hora, (-1, 1))\n",
    "\n",
    "\n",
    "    tfidf=features_stem_tfidf.transform([texto])\n",
    "    features=pca.transform(tfidf)\n",
    "\n",
    "    features=concatenate((features, wc), axis=1)\n",
    "    features=concatenate((features, hora), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    proba=clasificador.predict_proba(features)\n",
    "\n",
    "    proba_orig=proba\n",
    "    \n",
    "    proba=array([proba[0][0], #'emergencia'\n",
    "    proba[0][1], #Informacion\n",
    "    proba[0][2], #Nacimiento\n",
    "    proba[0][3]+proba[0][4], #otra\n",
    "    proba[0][5]+proba[0][6]+proba[0][7], #Pregunta\n",
    "    proba[0][8]]) #Respuesta\n",
    "    \n",
    "    pred=proba.argmax()\n",
    "    \n",
    "    conc=proba*100\n",
    "    conc=conc*conc\n",
    "    conc=conc.sum()\n",
    "\n",
    "\n",
    "    pred=proba.argmax()\n",
    "    max_proba=proba.max()\n",
    "    \n",
    "\n",
    "    \n",
    "    pred=str(vectorize(label_map.get)(pred))\n",
    "    \n",
    "    \n",
    "    if conc<minimo_conc:\n",
    "        pred='No_se_puede_asignar_etiqueta'\n",
    "    if proba[0]>0.01:\n",
    "        pred=pred+'-FLAG'\n",
    "    \n",
    "    out={'pred':pred,\n",
    "        'probabilidad_maxima':max_proba,\n",
    "        'indice_seguridad':conc, \n",
    "        'proba':proba}\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##THRESHOLD DE CONCENTRACIÃ“N. ABAJO DE ESTO NO SE PUEDE PREDECIR\n",
    "\n",
    "minimo_conc=4000\n",
    "\n",
    "##ETIQUETAS DE PREDICCIONES (NO CAMBIAR)\n",
    "label_map={0:'emergencia', 1:'informacion',\n",
    "           2:'nacimiento', 3:'otra',\n",
    "           4:'pregunta', 5:'respuesta'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pred': 'respuesta',\n",
       " 'probabilidad_maxima': 0.7692546,\n",
       " 'indice_seguridad': 6223.604,\n",
       " 'proba': array([0.00122801, 0.00984859, 0.01015188, 0.1697875 , 0.0397294 ,\n",
       "        0.7692546 ], dtype=float32)}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ID CONTACTO\n",
    "contact_uuid='fb82e199-ac49-41cd-a269-1654f29e180b'\n",
    "#TEXTO DEL MENSAJE\n",
    "texto= \"22.11.1997\"\n",
    "# TOKEN DEL API\n",
    "token=\"Token 0032fe79dbddceae3f4a86e86a726e16b88ec88e\"\n",
    "\n",
    "predice_modelo(contact_uuid, texto, token)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook funcion_modelo.ipynb to script\n",
      "[NbConvertApp] Writing 3961 bytes to funcion_modelo.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script funcion_modelo.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

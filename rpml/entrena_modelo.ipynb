{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import script_reglas\n",
    "import re \n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from numpy import reshape, shape, concatenate, nan\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leer archivo con mensajes, categorías y hora de entrada del mensaje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El archivo debe tener la siguiente estructura: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "respuestas_etiquetas=pd.read_csv('../datos/para_entrenamiento.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>hora_ultimo</th>\n",
       "      <th>texto</th>\n",
       "      <th>categ_opi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>La solicitud de enviar el SMS por Cobrar a 552...</td>\n",
       "      <td>otra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>Te llame y no pude localizarte. Tramita en lin...</td>\n",
       "      <td>otra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>59508</td>\n",
       "      <td>otra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>Mi bebe</td>\n",
       "      <td>nacimiento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>Hola si ya cada integrante tenemos fotos con l...</td>\n",
       "      <td>respuesta</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  hora_ultimo                                              texto  \\\n",
       "0           0           18  La solicitud de enviar el SMS por Cobrar a 552...   \n",
       "1           1           18  Te llame y no pude localizarte. Tramita en lin...   \n",
       "2           2           18                                              59508   \n",
       "3           3           18                                            Mi bebe   \n",
       "4           4           18  Hola si ya cada integrante tenemos fotos con l...   \n",
       "\n",
       "    categ_opi  \n",
       "0        otra  \n",
       "1        otra  \n",
       "2        otra  \n",
       "3  nacimiento  \n",
       "4   respuesta  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuestas_etiquetas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debe tener una columna de hora de entrada que se llamará hora_ultimo, el texto del mensaje (\"texto\") y la etiqueta asignada (\"categ_opi\"). En este archivo ya no deben estar los mensajes que sean clasificados por reglas (script_reglas.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target=respuestas_etiquetas['categ_opi'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texto=respuestas_etiquetas['texto'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definir funciones\n",
    "Se definen las funciones para procesar el texto y reducir palabras hasta su raíz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def procesa_texto(texto):\n",
    "    # Esta función manda todo a minúsculas, quita la segunda parte de las urls, quita puntuación y espacios finales\n",
    "    # Posteriormente, quita emojis del texto, quita acentos y ñs,\n",
    "    #calcula número de palabras y quita frases de apertura iniciales\n",
    "    \n",
    "    texto=texto.lower()\n",
    "    \n",
    "    \n",
    "    part=texto.partition('http') \n",
    "    part=part[0]+part[1]+' '+ ' '.join(part[2].split(' ')[1:])\n",
    "    texto=part\n",
    "    \n",
    "    part=texto.partition('bit.ly')\n",
    "    part=part[0]+part[1]+' '+ ' '.join(part[2].split(' ')[1:])\n",
    "    texto=part\n",
    "    \n",
    "    texto=re.sub('^[ \\t]+|[ \\t]+$', '', texto) \n",
    "\n",
    "    texto=re.sub('[^\\w\\s]','', texto)\n",
    "\n",
    "    texto=re.sub('^[ \\t]+|[ \\t]+$', '', texto)\n",
    "\n",
    "    texto=script_reglas.give_emoji_free_text(texto)\n",
    "\n",
    "    texto=unicodedata.normalize('NFD', texto).encode('ascii', 'ignore').decode('utf-8')\n",
    "    wc=len(str(texto).split(\" \"))\n",
    "    texto=re.sub('ola|buena noche|buenos dias|buenos dia|buen dia|buenas noches|buenas tardes|buenas tarde|buen dia|bien dia|buena tardes|buena tarde|saludos|hola','', texto)\n",
    "    texto=re.sub('\\n',' ', texto)\n",
    "    texto=re.sub('^[ \\t]+|[ \\t]+$', '', texto)\n",
    "    return texto , wc\n",
    "\n",
    "stemmer = SnowballStemmer(\"spanish\")\n",
    "\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    #Esta función separa el mensaje por palabras y reduce las palabras a su raíz\n",
    "    #\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicamos las funciones y dejamos en un array los features para el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texto_stem=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wc=[]\n",
    "\n",
    "for i in range(0, shape(train_texto)[0]):\n",
    "    wc.append(procesa_texto(train_texto[i])[1])\n",
    "    train_texto[i]=procesa_texto(train_texto[i])[0]\n",
    "    train_texto_stem.append(tokenize_and_stem(train_texto[i]))\n",
    "    train_texto_stem[i]=' '.join(train_texto_stem[i])\n",
    "wc=reshape(wc, (-1, 1))\n",
    "\n",
    "hora=respuestas_etiquetas.hora_ultimo.values\n",
    "hora=reshape(hora, (-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos stopwords que se van a quitar del cálculo de features del modelo. los pasamos a minúsculas y quitamos acentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop=nltk.corpus.stopwords.words(\"spanish\")\n",
    "\n",
    "for i in range(0, shape(stop)[0]):\n",
    "    stop[i]=unicodedata.normalize('NFD', stop[i]).encode('ascii', 'ignore').decode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering\n",
    "\n",
    "Convertimos el texto a una matriz de TFIDF. Cada columna representa un \"token\" del vocabulario completo (un token representa cada palabra y combinación de dos palabras). Si la celda es 0, es que no está esa palaba en el mensaje. Si es mayor a 0 sí está. Las palabras pesan menos cuanto más comunes sean en el vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(931, 466)"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=0.006, norm='l2', encoding='utf-8', ngram_range=([1, 2]),\n",
    "                        stop_words=stop)\n",
    "tfidf=tfidf.fit(train_texto_stem)\n",
    "\n",
    "features_stem = tfidf.transform(train_texto_stem)\n",
    "labels = respuestas_etiquetas.categ_opi\n",
    "features_stem.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay 931 mensajes con 466 palabras "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La matriz TFIDF tiene muchas columnas. Un modelo con tan pocas observaciones no puede ser entrenado con tantos features. Si reducimos las dimennsiones ganamos más poder predictivo al eliminar el ruido que puede sobreajustar el modelo. \n",
    "\n",
    "Para reducir las dimensiones usamos una técnica de \"latent semantic analysis\" que detecta temas subyacentes en los mensajes. Tras probar diferente número de factores, se definió que en 33 factores, la predicción no mejoraba sustancialmente por un factor adicional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_stem=features_stem.toarray()\n",
    "pca=TruncatedSVD(n_components=33)\n",
    "pca=pca.fit(features_stem, features_stem)\n",
    "features_stem_pca=pca.transform(features_stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se agrega a la matriz de dimensiones, los features de número de palabras y hora de entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=concatenate((features_stem_pca, wc), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=concatenate((x_train, hora), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definición de clasificador\n",
    "\n",
    "El clasificador que mejor funciona en este caso es el XGBoost. Éste es un ensamble de árboles que utiliza Gradient Descent para minimizar el error. Se cambia la métrica de error a Area Under Curve, para evitar que todas las predicciones se vayan a las clases más numerosas. Posteriormente, se entrena el modelo aplicando un .fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "clasificador=XGBClassifier(metrics='auc')\n",
    "clasificador=clasificador.fit(X=x_train,y=train_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['emergencia', 'informacion', 'nacimiento', 'otra', 'otra_queja',\n",
       "       'pregunta', 'pregunta_busca trabajo', 'pregunta_medica',\n",
       "       'respuesta'], dtype=object)"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clasificador.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Se exportan los PKL de matriz TFIDF, reducción de dimensiones y clasificador entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./modelo/modelo.pkl']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(tfidf, './modelo/mat_tfidf.pkl') #1\n",
    "joblib.dump(pca, './modelo/pca.pkl')  #2\n",
    "joblib.dump(clasificador, './modelo/modelo.pkl') # 3\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

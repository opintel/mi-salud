{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from sklearn.externals import joblib\n",
    "import re\n",
    "import script_reglas\n",
    "import unicodedata\n",
    "from numpy import reshape, concatenate, unique, vectorize, array\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_stem_tfidf=joblib.load('./modelo/mat_tfidf.pkl') #1\n",
    "pca=joblib.load('./modelo/pca.pkl')  #2\n",
    "clasificador=joblib.load('./modelo/modelo.pkl') # 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos los pkls que se generaron en entrenamiento modelo y les aplicamos las mismas funciones al texto nuevo para que entre igual que el texto con el que se entrenó. \n",
    "\n",
    "En la función de producción de modelo construimos la tabla de features en el mismo orden en el que entró al entrenamiento: 33 factores, word count y hora. La hora del nuevo mensaje saldrá de una petición GET al api de rapidpro. Se busca el último mensaje y extra su hora (en entero del 0 al 24). \n",
    "\n",
    "Se saca la probabilidad predicha de cada clase y se suman las probabilidades que van juntas. (Se agregaron categorías para ayudar a la clasificación de mensajes. Pregunta médica y pregunta busca trabajo son ambas preguntas, pero contienen mensajes muy diferentes). \n",
    "\n",
    "#### Se agrega: \n",
    "* Pregunta:\n",
    " + Búsqueda de trabajo\n",
    " + Preguntas médicas\n",
    " + Otras Preguntas  \n",
    "* Otra:\n",
    " + Quejas\n",
    " + Otras\n",
    " \n",
    "#### Índice de concentración\n",
    "\n",
    "El modelo calcula una predicción para todos los mensajes. Sin embargo, habrá veces que dos o más clases tienen alta probabilidad. Para controlar ese caso, se calcula un índice de concentración. Entre más alto, más concentrada está la probabilidad en una sola clase. Si el índice está por debajo de 4,000 el modelo no asignará una clase. El modelo asignará una clase a aproximadamente el 80% de los mensajes enviados.\n",
    "\n",
    "#### Flag de emergencias\n",
    "\n",
    "Los mensajes que sean emergencias y el modelo no prediga correctamente, son los errores más costosos. Por ello, si la probabilidad de emergencia es mayor a 1%, se le asignará la clase cuya probabildad sea la máxima. Sin embargo, se agregará un flag que indicará que el mensaje tiene una probabilidad alta de ser emergencia. Este flag se le asigna al ~8% de los mensajes y logra captar ~60% de las emergencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def procesa_texto(texto):\n",
    "    texto=texto.lower()\n",
    "    \n",
    "    \n",
    "    part=texto.partition('http')\n",
    "    part=part[0]+part[1]+' '+ ' '.join(part[2].split(' ')[1:])\n",
    "    texto=part\n",
    "    \n",
    "    part=texto.partition('bit.ly')\n",
    "    part=part[0]+part[1]+' '+ ' '.join(part[2].split(' ')[1:])\n",
    "    texto=part\n",
    "    \n",
    "    texto=re.sub('^[ \\t]+|[ \\t]+$', '', texto) \n",
    "\n",
    "    texto=re.sub('[^\\w\\s]','', texto)\n",
    "\n",
    "    texto=re.sub('^[ \\t]+|[ \\t]+$', '', texto)\n",
    "\n",
    "    texto=script_reglas.give_emoji_free_text(texto)\n",
    "\n",
    "    texto=unicodedata.normalize('NFD', texto).encode('ascii', 'ignore').decode('utf-8')\n",
    "    wc=len(str(texto).split(\" \"))\n",
    "    texto=re.sub('ola|buena noche|buenos dias|buenos dia|buen dia|buenas noches|buenas tardes|buenas tarde|buen dia|bien dia|buena tardes|buena tarde|saludos|hola','', texto)\n",
    "    texto=re.sub('\\n',' ', texto)\n",
    "    texto=re.sub('^[ \\t]+|[ \\t]+$', '', texto)\n",
    "    return texto , wc\n",
    "\n",
    "stemmer = SnowballStemmer(\"spanish\")\n",
    "\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "\n",
    "def predice_modelo(contact_uuid, texto, token):\n",
    "    response = requests.get(\"http://rapidpro.datos.gob.mx/api/v2/runs.json\",\n",
    "                            headers={\"Authorization\": token},\n",
    "                            params={\"contact\": contact_uuid})\n",
    "    response=json.loads(response.text)\n",
    "    hora=response['results'][0]['created_on']\n",
    "    hora=int(hora[11:13])\n",
    "\n",
    "    texto, wc =procesa_texto(texto)\n",
    "    \n",
    "    texto=tokenize_and_stem(texto)\n",
    "    texto=' '.join(texto)\n",
    "    \n",
    "    wc=reshape(wc, (-1, 1))\n",
    "    hora=reshape(hora, (-1, 1))\n",
    "\n",
    "\n",
    "    tfidf=features_stem_tfidf.transform([texto])\n",
    "    features=pca.transform(tfidf)\n",
    "\n",
    "    features=concatenate((features, wc), axis=1)\n",
    "    features=concatenate((features, hora), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    proba=clasificador.predict_proba(features)\n",
    "\n",
    "    proba_orig=proba\n",
    "    \n",
    "    proba=array([proba[0][0], #'emergencia'\n",
    "    proba[0][1], #Informacion\n",
    "    proba[0][2], #Nacimiento\n",
    "    proba[0][3]+proba[0][4], #otra\n",
    "    proba[0][5]+proba[0][6]+proba[0][7], #Pregunta\n",
    "    proba[0][8]]) #Respuesta\n",
    "    \n",
    "    pred=proba.argmax()\n",
    "    \n",
    "    conc=proba*100\n",
    "    conc=conc*conc\n",
    "    conc=conc.sum()\n",
    "\n",
    "\n",
    "    pred=proba.argmax()\n",
    "    max_proba=proba.max()\n",
    "    \n",
    "\n",
    "    \n",
    "    pred=str(vectorize(label_map.get)(pred))\n",
    "    \n",
    "    \n",
    "    if conc<minimo_conc:\n",
    "        pred='No_se_puede_asignar_etiqueta'\n",
    "    if proba[0]>0.01:\n",
    "        pred=pred+'-FLAG'\n",
    "    \n",
    "    out={'pred':pred,\n",
    "        'probabilidad_maxima':max_proba,\n",
    "        'indice_seguridad':conc, \n",
    "        'proba':proba}\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##THRESHOLD DE CONCENTRACIÓN. ABAJO DE ESTO NO SE PUEDE PREDECIR\n",
    "\n",
    "minimo_conc=4000\n",
    "\n",
    "##ETIQUETAS DE PREDICCIONES (NO CAMBIAR)\n",
    "label_map={0:'emergencia', 1:'informacion',\n",
    "           2:'nacimiento', 3:'otra',\n",
    "           4:'pregunta', 5:'respuesta'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'results'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-308a859f0983>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Token [inserta aqui]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mpredice_modelo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontact_uuid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-63-a8eaacba71c7>\u001b[0m in \u001b[0;36mpredice_modelo\u001b[0;34m(contact_uuid, texto, token)\u001b[0m\n\u001b[1;32m     47\u001b[0m                             params={\"contact\": contact_uuid})\n\u001b[1;32m     48\u001b[0m     \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mhora\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'results'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'created_on'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mhora\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhora\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'results'"
     ]
    }
   ],
   "source": [
    "#ID CONTACTO\n",
    "contact_uuid='fb82e199-ac49-41cd-a269-1654f29e180b'\n",
    "#TEXTO DEL MENSAJE\n",
    "texto= \"22.11.1997\"\n",
    "# TOKEN DEL API\n",
    "token=\"Token [inserta aqui]\"\n",
    "\n",
    "predice_modelo(contact_uuid, texto, token)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook funcion_modelo.ipynb to script\n",
      "[NbConvertApp] Writing 5545 bytes to funcion_modelo.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script funcion_modelo.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
